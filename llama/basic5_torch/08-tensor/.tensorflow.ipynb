{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [隨機梯度](http://zh.gluon.ai/chapter_optimization/gd-sgd.html)\n",
    "- [dense](https://keras-cn.readthedocs.io/en/latest/layers/core_layer/)\n",
    "- [mnist](https://sweetornotspicymarathon.medium.com/tesorflow-keras-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-%E6%96%B0%E6%89%8B%E4%B8%80%E5%AE%9A%E8%A6%81%E7%8E%A9%E7%9A%84mnist%E6%89%8B%E5%AF%AB%E6%95%B8%E5%AD%97%E8%BE%A8%E8%AD%98-9327366cc838) \n",
    "  每一張圖片為 28*28大小，這個數據庫當中包含60000筆訓練影像和10000筆測試影像\n",
    "\n",
    "## flattern vs dense\n",
    "\n",
    "### 以CNN為[例](https://medium.com/@vaibhav1403/flattening-in-neural-network-10e260d2b06f)\n",
    "1. Structure of CNNs:\n",
    "  CNNs usually have a series of convolutional layers followed by pooling layers. These layers produce multi-dimensional outputs known as feature maps.\n",
    "  To connect these feature maps to the fully connected layers (which are regular neural network layers with no spatial dimensions), we need to reshape or “flatten” these maps.\n",
    "1. Flatten Operation:\n",
    "  Suppose the output of your final pooling layer is a 3D tensor of shape (height, width, depth). The flatten operation will reshape this 3D tensor into a 1D tensor (vector) of shape (height * width * depth,).\n",
    "  For instance, if you have a 6x6 feature map with 64 channels (6x6x64 tensor), after flattening, you’d get a vector of 2,304 elements.\n",
    "1. Purpose:\n",
    "  The main reason for flattening is to prepare the data for the dense layers of the network. Dense layers require their input data in a flattened form, as they don’t work with the 2D spatial dimensions inherent in the feature maps.\n",
    "  After flattening, you often have one or more dense layers which perform the final computations and produce the output, whether it be a classification score, regression output, etc.\n",
    "1. Visualization:\n",
    "  Imagine you have a stack of feature maps, each one like a 2D grid of values. Flattening is like taking each of these grids, lining them up, and stringing them end-to-end into a long ribbon or vector.\n",
    "1. Note:\n",
    "  While flattening is a common practice in many classic CNN architectures, some modern architectures utilize global average pooling or other techniques to transition from spatial layers to dense layers without explicitly flattening.\n",
    "  In essence, flattening is the simple process of reshaping multi-dimensional data into a vector, making it compatible with fully connected layers in a neural network.\n",
    "\n",
    "![](tensorflow.files/flatter1.png)\n",
    "![](tensorflow.files/flattern3.png)\n",
    "![](tensorflow.files/flattern2.png)\n",
    "![](tensorflow.files/dense1.png)\n",
    "Dense層是使用線性代數裡的矩陣運算概念實現的。\n",
    "\n",
    "具體地說，Dense層的計算過程包括了對輸入和前一層輸出的矩陣乘法，以及對輸入和輸出的加權和。 這個綜合計算可以用下面這個矩陣的形式來表示\n",
    "\n",
    "y = W*x + b\n",
    "\n",
    "其中 y 是 Dense 層的輸出，x 是輸入，W 是 Dense 層的權重矩陣， b 是偏差項。\n",
    "\n",
    "這個運算滿足矩陣乘法的性質， W 可以看成是一個 m x n 的矩陣， x 是一個 n x 1 的向量, 這樣 y 就是一個 m x 1 的向量，就是輸出。這種運算方式可以幫助網絡在運算過程中組合特徵，輸出高維度且有用的特徵。\n",
    "  ![](tensorflow.files/dense2.png)\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "# Import TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    " \n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "class_names = metadata.features['label'].names\n",
    "print(\"Class names: {}\".format(class_names))\n",
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples:     {}\".format(num_test_examples))\n",
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32) # 整數轉浮點\n",
    "  images /= 255\n",
    "  return images, labels\n",
    " \n",
    "# The map function applies the normalize function to each element in the train\n",
    "# and test datasets 函數normalize 應用到每筆資料\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    " \n",
    "# The first time you use the dataset, the images will be loaded from disk\n",
    "# Caching will keep them in memory, making training faster\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()\n",
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((28,28))\n",
    " \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "#如果目標是one-hot 編碼，比如二分類【0,1】【1,0】，損失函數用 categorical_crossentropy。 如果目標是數字編碼 ，比如二分類0,1，損失函數用 sparse_categorical_crossentropy。\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE)\n",
    "model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    " \n",
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)\n",
    " \n",
    "for test_images, test_labels in test_dataset.take(1):\n",
    "  test_images = test_images.numpy()\n",
    "  test_labels = test_labels.numpy()\n",
    "  predictions = model.predict(test_images)\n",
    "np.argmax(predictions[0])\n",
    "```\n",
    "\n",
    "程式參考\n",
    "- [tfdataset](https://www.scaler.com/topics/tensorflow/data-pipelines-with-tensorflow-data-services/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop - eager execution\n",
    "```python\n",
    "optimizer = tf.train. MomentumOptimizer(...)\n",
    "for (x, y) in dataset make_one_shot_iterator(): \n",
    "  with tf.GradientTape() as g:\n",
    "    y_ = model(x)\n",
    "    loss=loss_fn(y, y_)\n",
    "  grads= g.gradient (y_, model variables)\n",
    "  optimizer.apply_gradients (zip (grads, model variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
