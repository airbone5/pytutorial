{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [æœ¬æ–‡åƒè€ƒ](https://www.linkedin.com/pulse/customizing-llama-3-guide-fine-tuning-your-dataset-ali-farahani-hllke)\n",
    " \n",
    " Llama-3 is one of the latest LLMs released by Meta under an open-source license. Its impressive performance in different tasks such as text-generation, translation, question answering, etc., position it as a strong competitor to closed-source models like GPT-4.\n",
    "\n",
    " The key advantages of Llama-3 include:\n",
    "\n",
    " - Llama-3 is an open-source LLM, you can use it and fine-tune it to your needs.\n",
    "\n",
    " - Llama-3 comes in two sizes: 8billion and 70billion parameters. The 70B version competes effectively with closed-source models like GPT-4.\n",
    "\n",
    " - Llama-3 is multi-lingual, means it understands different languages such as English, French, Persian, and many more.\n",
    "\n",
    " - While the Llama-3 is open-source, accessing it requires approval from Meta's Llama-3 AI team. This may present some limitations or barriers for certain users :(.\n",
    "\n",
    " These features make Llama-3 an ideal choice for users who want to adapt it to their specific tasks. This guide will walk you trough to use Llama-3 with 8 billion parameters, and fine-tune it with your custom dataset step-by-step. Note that we donâ€™t go through the technical details of Llama-3 and transformers, we simply utilize it.\n",
    "\n",
    " We use hugging face's transformers library in this text. The python packages you need are as follows:\n",
    "\n",
    " - Transformers: HuggingFace Transformer Library.\n",
    "\n",
    " - trl: HuggingFace Transformer Reinforcement Library (we use trl for fine-tuning Llama-3).\n",
    "\n",
    " - datasets: HuggingFace community-driven open-source library for datasets.\n",
    "\n",
    " - peft: HuggingFace Parameter-Efficient Fine-Tuning (peft is useful in cases we don't have powerful GPUs for fine-tuning)\n",
    "\n",
    " To install these packages you can simply use pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers trl datasets peft        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Defining Llama-3 model\n",
    "\n",
    " The first step is to create a Llama-3 model, download and load its weights. As mentioned earlier, official Llama-3 model is not publicly available. To get access to it you need to sign a form provided in this address (https://huggingface.co/meta-llama/Meta-Llama-3-8B).\n",
    "\n",
    " You can also use Llama-3 models shared by other users or organizations. These models are based on the official Llama-3, but they've been fine-tuned on specific datasets, potentially altering their capabilities.\n",
    "\n",
    " You can see a list of available Llama-3 models shared by community in HuggingFace models page(https://huggingface.co/models?sort=trending&search=llama-3).\n",
    "\n",
    " We use \"NousResearch/Meta-Llama-3-8B-Instruct\" model.\n",
    "\n",
    " The following codes creates and loads the model. If your computer has a CUDA capable GPU and the drivers are installed correctly, it will utilize your GPU.\n",
    "\n",
    " Quantization_config is used to reduce the memory needed for loading the model. The quantized version requires around 6GB of GPU memory to load. If you donâ€™t use quantization (by removing quantization_config=quantization_config line) the memory needed for loading the model increases to around 12GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77f041012af4656838d7a40535ac2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# å¦‚æœä¸æŒ‡å®šcache_dir å¯ä»¥åœ¨%userprofile%/.cacheä¸­\n",
    "\n",
    "# ä¸‹é¢æŒ‡å®šGPUçš„æ–¹æ³•,æˆ‘è¦ºå¾—æ²’ç”¨,ä½†æ˜¯ä¸ç¢ºå®š\n",
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"NousResearch/Meta-Llama-3-8B-Instruct\", \n",
    "#     quantization_config=quantization_config, \n",
    "#     device_map= device, #\"cuda:0\" or \"auto\", æˆ‘è¦ºå¾—æ²’ç”¨åˆ°GPU\n",
    "#     cache_dir='../../pretrain/'\n",
    "    \n",
    "# )\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NousResearch/Meta-Llama-3-8B-Instruct\", \n",
    "    quantization_config=quantization_config, \n",
    "    cache_dir='../../pretrain/'\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    cache_dir='../../pretrain/'\n",
    ")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128009,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this block of code, the model is downloaded from huggingface's model hub and copied to `~/.cache/huggingface/` directory (in Linux machines). Then the downloaded weights are loaded into your computers memory (based on device=gpu | cpu).\n",
    "\n",
    "## Using Llama-3\n",
    "\n",
    "Now you can use Llama-3. For using a LLM you first need to know its prompt template.\n",
    "\n",
    "Prompt template is a reusable structure that helps users craft effective prompts. These templates act as a blueprint, providing a framework to guide the LLM towards generating the desired output.\n",
    "\n",
    "Prompt templates typically include placeholders or slots where users can insert unique input data or instructions. This allows for a high degree of customization while maintaining the overall structure that has been optimized for the LLM's capabilities.\n",
    "\n",
    "The prompt template of Llama-3 is as follows (https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/):\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{CONTEXT}\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{QUESTION}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{CONTEXT} is additional information or background provided to the model alongside the main question.\n",
    "\n",
    "{QUESTION} is the question you ask from the LLM.\n",
    "```\n",
    "Example:\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an Artificial Intelligence assistant. Answer the questions in an academic style.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What are some advantages of ReLU activation function?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "Now you can define a prompt and pass it to Llama-3 for inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "c:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "åœ¨ä¿éšªæ³•è¦ä¸­ï¼Œå—ç›Šäººï¼ˆBeneficiaryï¼‰æ˜¯æŒ‡è¢«ç¹¼æ‰¿äººæˆ–è¢«è³ å„Ÿçš„å€‹äººæˆ–æ©Ÿæ§‹ï¼Œå—ç›Šæ–¼ä¿éšªå¥‘ç´„ä¸­çš„ä¿éšªé‡‘æˆ–è³ å„Ÿé‡‘ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œå—ç›Šäººæ˜¯æŒ‡è¢«æŒ‡å®šæˆ–æŒ‡å®šçš„å€‹äººæˆ–æ©Ÿæ§‹ï¼Œå°‡åœ¨ä¿éšªäººæ­»äº¡æˆ–å‚·äº¡æ™‚æˆ–å› ç‚ºä¿éšªäº‹æ•…è€Œç²å¾—ä¿éšªé‡‘æˆ–è³ å„Ÿé‡‘çš„å°è±¡ã€‚<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "tokenized_chat = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "\n",
    "generate_ids = model.generate(tokenized_chat, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(generate_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Result:\n",
    "\n",
    " The result of running the codes in a ipython session in vscode.\n",
    "\n",
    " As can be seen from the code block, we first organize our prompt in the correct tramplate. Then we use the tokenizer to convert the prompt text into a list of token_ids. Finally, we pass the tokenized prompt to model.generate() method. Tokenizer.decode() is used to convert token_ids to a string so we can print it to see the result.\n",
    "\n",
    "## Fine-tuning Llama-3\n",
    "\n",
    " Fine-tuning a HuggingFace model using the Transformers Reinforcement Learning (trl) library is a very simple process. The key requirement is to create a dataset that is compatible with the trl library. The dataset can be structured as a list of dictionaries, where each dictionary item has a \"text\" key. This \"text\" key should contain the input text that you want to use for fine-tuning the model.\n",
    "\n",
    " See the example bellow:\n",
    "\n",
    " PROMPT ç”šéº¼æ˜¯ä¿éšª\n",
    "RESPONSE ä¿éšªæŒ‡ç•¶äº‹äººç´„å®šï¼Œä¸€æ–¹äº¤ä»˜ä¿éšªè²»æ–¼ä»–æ–¹ï¼Œä»–æ–¹å°æ–¼å› ä¸å¯é æ–™ï¼Œæˆ–ä¸å¯æŠ—åŠ›ä¹‹äº‹æ•…æ‰€è‡´ä¹‹æå®³ï¼Œè² æ“”è³ å„Ÿè²¡ç‰©ä¹‹è¡Œç‚º\n",
    "PROMPT ç”šéº¼æ˜¯ä¿éšªäºº\n",
    "RESPONSE ä¿éšªäººæŒ‡ç¶“ç‡Ÿä¿éšªäº‹æ¥­ä¹‹å„ç¨®çµ„ç¹”ï¼Œåœ¨ä¿éšªå¥‘ç´„æˆç«‹æ™‚ï¼Œæœ‰ä¿éšªè²»ä¹‹è«‹æ±‚æ¬Šï¼›åœ¨æ‰¿ä¿å±éšªäº‹æ•…ç™¼ç”Ÿæ™‚ï¼Œä¾å…¶æ‰¿ä¿ä¹‹è²¬ä»»ï¼Œè² æ“”è³ å„Ÿä¹‹ç¾©å‹™ã€‚         \n",
    "PROMPT ç”šéº¼æ˜¯è¢«ä¿éšªäºº\n",
    "RESPONSE è¢«ä¿éšªäººï¼ŒæŒ‡æ–¼ä¿éšªäº‹æ•…ç™¼ç”Ÿæ™‚ï¼Œé­å—æå®³ï¼Œäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼›è¦ä¿äººäº¦å¾—ç‚ºè¢«ä¿éšªäººã€‚\n",
    "PROMPT ç”šéº¼æ˜¯å—ç›Šäºº\n",
    "RESPONSE å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäººã€‚\n",
    "PROMPT ç”šéº¼æ˜¯ä¿éšªæ¥­\n",
    "RESPONSE ä¿éšªæ¥­ï¼ŒæŒ‡ä¾æœ¬æ³•çµ„ç¹”ç™»è¨˜ï¼Œä»¥ç¶“ç‡Ÿä¿éšªç‚ºæ¥­ä¹‹æ©Ÿæ§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{answer}<|eot_id|>\"\"\"\n",
    "\n",
    "data = [\n",
    "    {\"text\": TEMPLATE.format(context=\"ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†,ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\", question=\"ç”šéº¼æ˜¯ä¿éšª\", answer=\"ä¿éšªæŒ‡ç•¶äº‹äººç´„å®šï¼Œä¸€æ–¹äº¤ä»˜ä¿éšªè²»æ–¼ä»–æ–¹ï¼Œä»–æ–¹å°æ–¼å› ä¸å¯é æ–™ï¼Œæˆ–ä¸å¯æŠ—åŠ›ä¹‹äº‹æ•…æ‰€è‡´ä¹‹æå®³ï¼Œè² æ“”è³ å„Ÿè²¡ç‰©ä¹‹è¡Œç‚º\")},\n",
    "    {\"text\": TEMPLATE.format(context=\"ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†,ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\", question=\"ç”šéº¼æ˜¯ä¿éšªäºº\", answer=\"ä¿éšªäººæŒ‡ç¶“ç‡Ÿä¿éšªäº‹æ¥­ä¹‹å„ç¨®çµ„ç¹”ï¼Œåœ¨ä¿éšªå¥‘ç´„æˆç«‹æ™‚ï¼Œæœ‰ä¿éšªè²»ä¹‹è«‹æ±‚æ¬Šï¼›åœ¨æ‰¿ä¿å±éšªäº‹æ•…ç™¼ç”Ÿæ™‚ï¼Œä¾å…¶æ‰¿ä¿ä¹‹è²¬ä»»ï¼Œè² æ“”è³ å„Ÿä¹‹ç¾©å‹™ã€‚\")},\n",
    "    {\"text\": TEMPLATE.format(context=\"ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†,ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\", question=\"ç”šéº¼æ˜¯è¢«ä¿éšªäºº\", answer=\"è¢«ä¿éšªäººï¼ŒæŒ‡æ–¼ä¿éšªäº‹æ•…ç™¼ç”Ÿæ™‚ï¼Œé­å—æå®³ï¼Œäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼›è¦ä¿äººäº¦å¾—ç‚ºè¢«ä¿éšªäººã€‚\")},\n",
    "    {\"text\": TEMPLATE.format(context=\"ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†,ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\", question=\"ç”šéº¼æ˜¯å—ç›Šäºº\", answer=\"å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäººã€‚\")},\n",
    "    {\"text\": TEMPLATE.format(context=\"ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†,ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\", question=\"ç”šéº¼æ˜¯ä¿éšªæ¥­\", answer=\"ä¿éšªæ¥­ï¼ŒæŒ‡ä¾æœ¬æ³•çµ„ç¹”ç™»è¨˜ï¼Œä»¥ç¶“ç‡Ÿä¿éšªç‚ºæ¥­ä¹‹æ©Ÿæ§‹\")},\n",
    "\n",
    "]\n",
    "\n",
    "my_dataset = Dataset.from_list(data)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the above code block, we defined a TEMPLATE, then we created a list of dictionaries. each item in the dictionaries has a \"text\" key.\n",
    "\n",
    " Dataset class has methods for creating dataset from different sources such as CSV file, Pandas, list, json, etc.\n",
    "\n",
    " ** If you are familiar with pytorch's Dataset class, you can define a custom class derived from torch.util.data.Dataset class and override its __getitem__ and __len__ methods. The __getitem__ method should return a value of type dict with a \"text\" key.\n",
    "\n",
    " After creating your dataset, you can start training your model. Here is the code for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_num_proc. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:314: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60d3287a3e2416082033d372bb51017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5422e4b13142c6a84a835b36474334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5391, 'grad_norm': 5.759950637817383, 'learning_rate': 0.00018947368421052632, 'epoch': 10.0}\n",
      "{'loss': 1.2907, 'grad_norm': 1.3779449462890625, 'learning_rate': 0.00016842105263157895, 'epoch': 20.0}\n",
      "{'loss': 0.4464, 'grad_norm': 0.5165285468101501, 'learning_rate': 0.00014736842105263158, 'epoch': 30.0}\n",
      "{'loss': 0.3223, 'grad_norm': 0.15940603613853455, 'learning_rate': 0.0001263157894736842, 'epoch': 40.0}\n",
      "{'loss': 0.305, 'grad_norm': 0.11167210340499878, 'learning_rate': 0.00010526315789473685, 'epoch': 50.0}\n",
      "{'loss': 0.2916, 'grad_norm': 0.17236745357513428, 'learning_rate': 8.421052631578948e-05, 'epoch': 60.0}\n",
      "{'loss': 0.2798, 'grad_norm': 0.13287067413330078, 'learning_rate': 6.31578947368421e-05, 'epoch': 70.0}\n",
      "{'loss': 0.2688, 'grad_norm': 0.13519032299518585, 'learning_rate': 4.210526315789474e-05, 'epoch': 80.0}\n",
      "{'loss': 0.26, 'grad_norm': 0.12910650670528412, 'learning_rate': 2.105263157894737e-05, 'epoch': 90.0}\n",
      "{'loss': 0.2545, 'grad_norm': 0.12832623720169067, 'learning_rate': 0.0, 'epoch': 100.0}\n",
      "{'train_runtime': 191.1643, 'train_samples_per_second': 16.74, 'train_steps_per_second': 0.523, 'train_loss': 0.7258142971992493, 'epoch': 100.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.7258142971992493, metrics={'train_runtime': 191.1643, 'train_samples_per_second': 16.74, 'train_steps_per_second': 0.523, 'total_flos': 2140846018560000.0, 'train_loss': 0.7258142971992493, 'epoch': 100.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "\n",
    "Lora_config = LoraConfig(r=16,lora_alpha=32,lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "\n",
    "#model = TFAutoModel.from_pretrained(pretrained_weights)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id  \n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    peft_config=Lora_config,\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = my_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 256,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 100, #100ğŸ˜‰\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_torch\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1234,\n",
    "        output_dir = \"checkpoints\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RTX 2060 èŠ±äº†159.54 åˆ†é˜; 4090 3åˆ†20\n",
    "\n",
    "æ³¨æ„:\n",
    "1. If you get an error during training it might be because of not setting pad_token for the model and tokenizer. For solving this issue, simply add these two lines after model and tokenizer definition:\n",
    "  ```python\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model.config.pad_token_id = model.config.eos_token_id  \n",
    "  ```\n",
    "1. SFTTrainer vs Trainer\n",
    "[åƒè€ƒ](https://medium.com/@sujathamudadla1213/difference-between-trainer-class-and-sfttrainer-supervised-fine-tuning-trainer-in-hugging-face-d295344d73f7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " after some time the training is done.\n",
    "\n",
    " As mentioned earlier, peft is used to decrease the GPU memory needed for fine-tuning the model. Without using peft you would need more than 16GB of memory. The GPU utilization of running the above code is shown in the following image (Tesla T4):\n",
    "\n",
    " To test if the model learned your data you can ask it a question from your custom dataset (instructions in section 2).\n",
    "\n",
    " Llama-3's impressive capabilities come with a potential trade-off. Like many large language models, it is vulnerable to catastrophic forgetting. This means that extensive fine-tuning on a specific task can cause the model to lose some of the general knowledge it acquired during its pre-training on the massive Meta dataset. To lower this risk, consider a cautious approach to fine-tuning, balancing the benefits of task-specific adaptation with the potential loss of pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäººã€‚<|eot_id|>\n",
      "system\n",
      "\n",
      "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
      "\n",
      "user\n",
      "\n",
      "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
      "\n",
      "assistant\n",
      "\n",
      "å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäººã€‚\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "tokenized_chat = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_chat = tokenized_chat.to(device)\n",
    "\n",
    "generate_ids = model.generate(tokenized_chat, max_new_tokens=256)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(generate_ids[0]))\n",
    "print(tokenizer.decode(generate_ids[0],skip_special_tokens = True)) #ğŸ˜‰skip_special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"tmp/xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"tmp/newmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa38ad6fa11453f893bac925c9928ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amodel = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tmp/newmodel\", \n",
    "    local_files_only=True,\n",
    "    quantization_config=quantization_config, \n",
    "    device_map= device, #\"cuda:0\" or \"auto\",\n",
    "    cache_dir='../../pretrain/'\n",
    "    \n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "ä½ æ˜¯ä¿éšªæ³•è¦åŠ©ç†ã€‚ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚\n",
      "\n",
      "user\n",
      "\n",
      "ç”šéº¼æ˜¯å—ç›Šäºº?\n",
      "\n",
      "assistant\n",
      "\n",
      "å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäººã€‚\n"
     ]
    }
   ],
   "source": [
    "#å—ç›Šäººï¼ŒæŒ‡è¢«ä¿éšªäººæˆ–è¦ä¿äººç´„å®šäº«æœ‰è³ å„Ÿè«‹æ±‚æ¬Šä¹‹äººï¼Œè¦ä¿äººæˆ–è¢«ä¿éšªäººå‡å¾—ç‚ºå—ç›Šäºº\n",
    "print(tokenizer.decode(generate_ids[0],skip_special_tokens = True)) #ğŸ˜‰skip_special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa4328e725f442aa65f107ca319e3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in tmp/newmodel. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m amodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/newmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     16\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m ) \u001b[38;5;66;03m#.to(device)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmp/newmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#cache_dir='../../pretrain/'\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;66;03m# .to(device) \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(tokenizer.decode(generate_ids[0],skip_special_tokens = True)) #ğŸ˜‰skip_special_token\u001b[39;00m\n",
      "File \u001b[1;32mc:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:877\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1049\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[0;32m   1047\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1053\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized model in tmp/newmodel. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "#from trl import SFTTrainer\n",
    "#from transformers import TrainingArguments\n",
    "#from peft import LoraConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "amodel = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tmp/newmodel\", \n",
    "    local_files_only=True,\n",
    "    quantization_config=quantization_config, \n",
    "    device_map= device, #\"cuda:0\" or \"auto\",\n",
    "    cache_dir='../../pretrain/'\n",
    "    \n",
    ") #.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tmp/newmodel\"\n",
    "    #cache_dir='../../pretrain/'\n",
    ")# .to(device) \n",
    "# print(tokenizer.decode(generate_ids[0],skip_special_tokens = True)) #ğŸ˜‰skip_special_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶ä»–åƒè€ƒ\n",
    "- [Accelerate Big Model Inference: How Does it Work?](https://www.youtube.com/watch?v=MWCSGj9jEAo&t=2s&ab_channel=HuggingFace)\n",
    "- [Fine Tuning TinyLlama on Custom Dataset | Large Language Models (LLMs)](https://www.youtube.com/watch?v=3SlpXBvIqNw)\n",
    "\n",
    "- [How large language models work, a visual intro to transformers | Chapter 5, Deep Learning](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "- [Understanding Model Loading in Diffusers](https://medium.com/@dicksongoodluck123/understanding-model-loading-in-diffusers-db63f7ba562e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
