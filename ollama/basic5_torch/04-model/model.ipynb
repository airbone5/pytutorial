{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加載與保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "# 本地\n",
    "#location='../../pretrain/rbt3'\n",
    "# 遠端\n",
    "location='hfl/rbt3'\n",
    "import torch\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在線加載\n",
    "```python\n",
    "model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True).to(device)\n",
    "```\n",
    "## 模型下載\n",
    "```python\n",
    "!git clone \"https://huggingface.co/hfl/rbt3\"\n",
    "!git lfs clone \"https://huggingface.co/hfl/rbt3\" --include=\"*.bin\"\n",
    "```\n",
    "## 離線加載\n",
    "```\n",
    "model = AutoModel.from_pretrained(\"../../pretrain/rbt3\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加載參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(location).to(device) #\"../../pretrain/rbt3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/rbt3\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(location)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型調用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6710025072097778}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果直接就要使用,pipeline 就行 \n",
    "from transformers import pipeline\n",
    "gen=pipeline(\"text-classification\",location,device=device)\n",
    "sen = \"弱小的我也有大夢想！\"\n",
    "gen(sen)\n",
    "sen = \"饭菜有些咸！\"\n",
    "gen(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2483, 2207, 4638, 2769,  738, 3300, 1920, 1918, 2682, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"弱小的我也有大夢想！\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(location)\n",
    "inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不帶Model Head的模型調用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(location, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.8812,  0.9352,  0.5495,  ..., -0.5131,  0.9547,  0.1613],\n",
       "         [-0.5624, -0.1943,  0.3573,  ..., -0.1176, -0.4833, -0.4479],\n",
       "         [ 0.1194,  0.6461, -0.0019,  ..., -0.4652,  0.4643, -0.4450],\n",
       "         ...,\n",
       "         [ 0.2789,  0.2843, -0.2725,  ...,  0.0100,  0.5123, -0.0343],\n",
       "         [ 0.4851,  0.2916, -0.3561,  ..., -0.2059,  0.3635, -0.1149],\n",
       "         [ 0.8771,  0.9381,  0.5470,  ..., -0.5125,  0.9530,  0.1641]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.9194e-01, -8.3136e-01, -1.0000e+00, -9.8282e-01,  4.9045e-01,\n",
       "          7.3040e-02,  1.7596e-01,  2.7832e-01,  9.9510e-01,  9.9998e-01,\n",
       "         -7.5305e-02, -1.0000e+00, -4.0089e-02,  9.9940e-01, -1.0000e+00,\n",
       "          9.9998e-01,  9.5928e-01,  9.7920e-01, -9.9934e-01, -1.4762e-01,\n",
       "         -9.9442e-01, -7.9461e-01,  1.2178e-01,  9.7069e-01,  9.9981e-01,\n",
       "         -9.9928e-01, -9.9999e-01,  2.4507e-01, -9.5591e-01, -9.9998e-01,\n",
       "         -9.9922e-01, -9.9998e-01,  1.6468e-03, -1.4554e-01,  9.9765e-01,\n",
       "         -9.9033e-01,  3.2101e-02, -9.9628e-01, -9.9976e-01, -9.9985e-01,\n",
       "          9.3738e-02,  9.9786e-01,  1.6533e-01,  1.0000e+00,  2.4801e-01,\n",
       "          5.6161e-02,  9.9998e-01,  9.9079e-01,  6.2871e-03, -8.4375e-01,\n",
       "          4.9461e-03, -1.3205e-01, -9.9294e-01,  9.9756e-01,  3.5763e-01,\n",
       "          3.3414e-02,  9.9957e-01, -1.0000e+00, -9.9995e-01,  9.9545e-01,\n",
       "         -9.9991e-01,  9.9738e-01,  9.9228e-01,  9.9997e-01, -4.7149e-01,\n",
       "          9.9993e-01,  9.9990e-01,  9.1053e-01, -2.6676e-01, -1.0000e+00,\n",
       "          6.2756e-01, -9.5114e-01, -9.9998e-01,  1.5412e-01, -1.6173e-01,\n",
       "         -9.9997e-01,  9.8945e-01, -2.6479e-01,  9.9999e-01,  2.6068e-01,\n",
       "         -9.9998e-01,  3.4891e-02,  2.3367e-01,  1.0520e-01,  9.9996e-01,\n",
       "          9.9993e-01,  9.5067e-02, -9.8161e-01, -1.9703e-01, -9.9944e-01,\n",
       "         -5.7496e-01,  9.9965e-01,  9.9999e-01, -9.9999e-01,  9.9997e-01,\n",
       "         -8.3841e-01, -7.9462e-02, -6.6918e-03, -9.9875e-01,  9.9819e-01,\n",
       "         -1.2347e-03, -1.1421e-01,  1.0000e+00,  9.9996e-01, -6.1475e-01,\n",
       "         -9.9998e-01, -9.9902e-01,  9.9998e-01, -9.9991e-01,  3.2357e-01,\n",
       "          1.0000e+00,  9.7535e-01,  1.0000e+00,  9.9948e-01,  9.9999e-01,\n",
       "         -9.9999e-01, -3.0278e-01,  2.2743e-01, -9.9998e-01,  9.9981e-01,\n",
       "         -9.9312e-01, -2.9454e-01, -9.3160e-01, -1.6878e-01, -1.2715e-01,\n",
       "         -1.0000e+00, -1.1202e-01, -5.4874e-02, -9.9366e-01, -9.9656e-01,\n",
       "         -9.9985e-01, -9.9988e-01,  9.9948e-01,  5.5707e-01,  2.7548e-01,\n",
       "         -1.3505e-01,  1.0318e-01,  9.2334e-03, -1.0000e+00, -9.9971e-01,\n",
       "         -9.9998e-01,  9.9232e-01,  5.3328e-01,  9.9487e-01, -9.9928e-01,\n",
       "          9.9999e-01, -9.9999e-01,  9.9997e-01,  9.9086e-01, -3.8917e-01,\n",
       "         -9.9002e-01, -5.0156e-02, -9.9981e-01, -7.0503e-03,  2.7595e-02,\n",
       "          9.8993e-01,  9.9753e-01,  9.1635e-01, -9.8751e-01,  1.0000e+00,\n",
       "         -9.9982e-01,  9.3482e-01,  1.2045e-01,  9.9971e-01,  1.0000e+00,\n",
       "         -1.0000e+00,  2.6124e-01, -1.0000e+00,  3.0002e-01, -6.3448e-03,\n",
       "          9.9998e-01,  9.9885e-01,  9.9172e-01,  9.9999e-01, -9.6245e-01,\n",
       "         -9.9990e-01,  9.9573e-01, -1.0000e+00,  9.4587e-01,  1.0000e+00,\n",
       "          6.6307e-02, -5.6973e-02,  9.9997e-01, -3.3465e-01,  9.9829e-01,\n",
       "         -3.6551e-02, -7.0445e-02, -9.9980e-01,  4.2611e-02,  9.3111e-01,\n",
       "          9.9815e-01, -9.9958e-01,  4.5502e-02,  9.9109e-01, -1.0904e-01,\n",
       "          9.5955e-02, -9.9986e-01, -9.9996e-01,  9.9997e-01,  9.9988e-01,\n",
       "          8.7461e-01, -2.7335e-01,  1.0000e+00,  4.4074e-01,  9.9994e-01,\n",
       "         -1.0739e-01,  6.1026e-01, -6.2272e-02,  9.9996e-01,  1.0958e-01,\n",
       "          9.0025e-01,  1.1001e-01,  9.9998e-01, -8.5826e-01, -1.0000e+00,\n",
       "          2.3313e-02,  9.7783e-01,  1.2612e-01, -9.9937e-01,  5.7102e-01,\n",
       "         -4.8980e-01,  9.9999e-01,  2.5081e-01, -9.5564e-01,  9.9929e-01,\n",
       "         -9.9992e-01,  7.7686e-01, -1.0000e+00, -9.9906e-01,  9.9999e-01,\n",
       "         -5.9380e-02, -1.0000e+00,  9.0231e-01,  9.9999e-01, -8.2361e-01,\n",
       "          9.9849e-01, -2.8901e-01, -9.9997e-01,  4.3155e-03,  2.4049e-02,\n",
       "         -1.0000e+00, -9.8277e-01, -1.0000e+00, -6.9985e-01, -1.2367e-01,\n",
       "          9.9140e-01, -1.0000e+00,  4.7901e-02, -9.9998e-01,  9.9931e-01,\n",
       "          9.9995e-01, -1.5530e-01,  7.4580e-02,  1.0000e+00, -6.0495e-01,\n",
       "          1.2907e-01, -9.5775e-01,  2.0764e-01, -5.8583e-02,  9.9838e-01,\n",
       "          3.2652e-01,  9.9919e-01, -9.9986e-01,  2.9553e-01,  9.5508e-01,\n",
       "         -1.0000e+00, -8.1005e-02,  9.9680e-01,  7.6370e-02,  9.6196e-02,\n",
       "         -1.9805e-01, -2.2939e-02,  8.2743e-01,  9.9976e-01,  1.0000e+00,\n",
       "          9.9675e-01,  1.0000e+00,  1.0000e+00, -2.7030e-01, -9.9922e-01,\n",
       "         -5.2451e-01,  1.9012e-01, -1.0000e+00, -9.8168e-01, -9.9987e-01,\n",
       "          9.9753e-01,  3.1232e-02,  1.0000e+00, -1.0000e+00,  1.0000e+00,\n",
       "         -9.6922e-01, -3.3792e-02, -9.5894e-02,  5.5688e-02, -4.1226e-01,\n",
       "         -2.1190e-01,  9.9957e-01, -7.5764e-02,  9.9851e-01,  9.9533e-01,\n",
       "          5.7556e-02, -1.0869e-01,  8.7788e-02, -2.3319e-01,  9.9917e-01,\n",
       "         -9.9868e-01,  9.9922e-01, -1.5305e-03,  9.9994e-01, -3.9893e-01,\n",
       "         -1.0000e+00,  9.9928e-01, -9.9998e-01, -3.6360e-01, -9.9997e-01,\n",
       "         -9.8091e-01,  1.4252e-01, -9.8546e-01,  1.6799e-01,  9.9309e-01,\n",
       "          3.3438e-01,  1.6205e-01, -1.0000e+00, -9.8847e-01,  9.8349e-01,\n",
       "          9.9996e-01, -9.9843e-01,  9.9770e-01,  9.9838e-01, -9.5865e-01,\n",
       "          2.8144e-01,  9.3631e-01, -9.9961e-01,  9.8151e-01, -1.0000e+00,\n",
       "          1.7169e-01,  9.9968e-01, -2.6020e-01, -9.9990e-01, -9.7765e-01,\n",
       "         -1.9476e-02,  8.7742e-01, -8.1681e-03,  1.0000e+00,  6.8386e-02,\n",
       "          2.1651e-01, -9.9999e-01, -9.9603e-01, -8.4852e-01,  4.0222e-01,\n",
       "          9.9894e-01, -9.9999e-01,  1.3283e-01,  9.9686e-01, -9.9988e-01,\n",
       "         -3.2614e-01,  3.9371e-02, -2.5423e-02, -1.4588e-01, -9.9884e-01,\n",
       "         -1.0000e+00, -9.9999e-01,  9.9999e-01,  9.8838e-02, -7.6641e-02,\n",
       "          9.9968e-01,  9.9999e-01,  9.9998e-01, -9.7870e-01,  9.7686e-01,\n",
       "          9.9991e-01, -3.5705e-01,  2.4070e-01, -9.8493e-01,  1.4791e-01,\n",
       "         -4.3730e-01, -9.9922e-01,  1.2192e-01,  6.1488e-01, -9.1708e-01,\n",
       "         -9.8413e-01,  1.0000e+00,  9.9997e-01,  1.0000e+00,  6.1276e-02,\n",
       "         -9.7927e-01,  9.3824e-01, -9.7012e-01, -9.9977e-01, -1.4190e-01,\n",
       "          9.9944e-01, -9.9682e-01,  1.7624e-01, -4.2597e-01, -9.3565e-01,\n",
       "          9.9880e-01, -9.9987e-01, -4.0541e-02, -1.0000e+00, -9.9998e-01,\n",
       "         -9.9996e-01,  1.0000e+00,  1.2388e-01, -1.6932e-01, -9.9974e-01,\n",
       "          1.0000e+00,  9.8590e-01, -9.8730e-01, -2.9300e-01,  9.9996e-01,\n",
       "         -3.7296e-01, -9.4321e-01, -1.0000e+00, -9.9969e-01,  9.9842e-01,\n",
       "          1.4252e-01,  1.0000e+00,  9.3824e-01, -9.9876e-01,  9.7759e-01,\n",
       "          9.5848e-01,  1.8950e-01, -9.9987e-01, -9.9971e-01, -4.6395e-02,\n",
       "          4.5792e-02, -5.0214e-02,  2.0804e-01, -9.5089e-02,  9.9999e-01,\n",
       "          3.9007e-02, -1.6569e-02, -2.4598e-01,  1.0000e+00,  7.8577e-01,\n",
       "         -9.9959e-01, -7.1981e-01, -6.8947e-02, -9.9545e-01, -9.9654e-01,\n",
       "         -9.9999e-01, -1.6149e-01,  1.8743e-01,  1.8045e-02, -9.8238e-01,\n",
       "         -9.9797e-01, -9.9973e-01, -1.3811e-01, -9.6996e-01, -9.9945e-01,\n",
       "         -1.2999e-01, -9.9396e-01, -9.9996e-01,  9.9999e-01, -9.9997e-01,\n",
       "         -1.0619e-01,  9.5270e-01,  9.9983e-01, -9.9991e-01,  2.4704e-01,\n",
       "          9.9836e-01, -9.9985e-01,  1.8684e-01, -9.7999e-01,  1.9549e-02,\n",
       "         -9.2749e-01, -1.0000e+00,  3.0701e-01,  9.9997e-01,  9.9997e-01,\n",
       "          9.9931e-01,  8.5743e-01, -6.4542e-01,  9.6942e-01,  9.9993e-01,\n",
       "          1.0000e+00, -1.2357e-02, -6.0748e-02, -1.0000e+00, -3.7737e-01,\n",
       "          9.4565e-01,  8.7580e-04,  9.0075e-01, -9.9991e-01,  3.9368e-01,\n",
       "         -9.8814e-01,  3.1502e-01,  1.0000e+00,  9.9745e-01,  4.3536e-01,\n",
       "         -1.0000e+00,  4.3523e-02, -5.9106e-01,  2.4907e-01, -9.9598e-01,\n",
       "         -7.4839e-02,  1.0000e+00, -9.8437e-01,  4.4978e-01, -9.9975e-01,\n",
       "         -9.9994e-01,  9.9998e-01, -9.9997e-01,  1.0000e+00,  9.7406e-01,\n",
       "         -9.9653e-01, -2.8799e-01, -9.9677e-01, -1.3305e-01, -7.0371e-02,\n",
       "         -2.0936e-02, -6.3869e-01, -1.1157e-01, -1.0000e+00,  5.1450e-02,\n",
       "          9.9168e-01, -2.4840e-02, -1.2494e-01, -9.9992e-01,  2.2636e-01,\n",
       "          9.5966e-01, -9.9992e-01, -9.9997e-01,  8.6413e-02, -2.9753e-01,\n",
       "          1.0604e-02,  4.4930e-01,  2.8699e-03,  1.1761e-01, -9.9837e-01,\n",
       "          9.8145e-02,  9.3438e-01, -9.9820e-01,  8.0846e-01, -7.4136e-01,\n",
       "         -8.4470e-01, -1.1708e-01,  9.9996e-01,  9.9997e-01, -9.9985e-01,\n",
       "         -9.9982e-01, -6.8143e-01,  1.2191e-01, -8.8743e-01,  9.9977e-01,\n",
       "          6.7937e-02, -9.9890e-01,  1.2936e-01,  1.1349e-01, -9.0332e-02,\n",
       "         -9.6823e-01,  1.0000e+00, -9.9845e-01,  1.0000e+00, -1.0000e+00,\n",
       "         -9.9503e-01,  1.3037e-01,  9.9998e-01, -1.0000e+00, -1.6132e-01,\n",
       "          9.9994e-01, -1.0000e+00,  1.0019e-01, -9.6825e-01,  9.6919e-01,\n",
       "          1.1456e-01,  3.4820e-03,  8.3226e-01, -9.9995e-01,  2.4348e-01,\n",
       "         -9.9995e-01,  9.6938e-01,  9.9858e-01, -9.9996e-01, -5.0371e-01,\n",
       "         -9.9995e-01, -1.2380e-02,  1.3494e-01, -9.9837e-01,  9.9807e-01,\n",
       "          9.9999e-01,  5.4418e-03,  9.6038e-01, -9.9992e-01,  5.8812e-02,\n",
       "         -2.7826e-01, -9.8766e-01,  1.4624e-02, -9.9994e-01,  9.8225e-01,\n",
       "         -9.9647e-01,  9.9940e-01, -1.0000e+00,  9.7563e-01,  9.9921e-01,\n",
       "         -9.1701e-02, -6.2932e-01, -9.3621e-02,  8.0738e-01, -9.9999e-01,\n",
       "         -2.7177e-01, -9.9996e-01, -9.9993e-01,  3.3151e-01,  9.9994e-01,\n",
       "          9.9650e-01,  5.0275e-02,  9.8908e-01, -9.9280e-01,  1.0574e-01,\n",
       "          3.5290e-01,  3.4078e-01,  1.0000e+00, -9.9992e-01, -9.9983e-01,\n",
       "          9.9866e-01, -9.9996e-01, -9.9739e-01,  1.0000e+00,  7.4752e-01,\n",
       "          9.9980e-01,  4.5181e-02, -9.9987e-01,  1.0520e-01,  4.9210e-02,\n",
       "          9.9998e-01,  2.3058e-02, -1.3613e-01,  1.0000e+00, -1.0548e-01,\n",
       "          6.4408e-02, -7.3896e-01,  9.9611e-01, -2.5295e-01, -5.9017e-02,\n",
       "          9.9907e-01, -9.4144e-01, -9.9955e-01, -9.9993e-01, -3.5933e-02,\n",
       "          4.1567e-02, -1.6425e-02,  8.8890e-02,  7.6155e-01,  1.0000e+00,\n",
       "         -9.9964e-01,  9.9564e-01, -1.0000e+00, -9.9988e-01,  1.1578e-01,\n",
       "         -2.4766e-02,  9.9978e-01, -1.8991e-01, -9.9352e-01,  1.8888e-01,\n",
       "         -9.8412e-01,  9.9928e-01, -9.9994e-01,  9.7035e-01, -3.6262e-02,\n",
       "          1.8432e-01,  9.9999e-01,  9.9852e-01, -1.0383e-01, -9.9900e-01,\n",
       "         -6.6239e-01, -7.8718e-02, -9.9846e-01,  9.9993e-01,  8.2822e-02,\n",
       "          3.3579e-01,  8.2330e-02,  1.0261e-02, -9.9999e-01, -9.9101e-01,\n",
       "          1.0794e-01,  9.9992e-01, -9.9999e-01,  9.9862e-01, -9.8445e-01,\n",
       "          9.9997e-01,  9.9995e-01,  1.0000e+00,  2.2601e-02,  9.9423e-01,\n",
       "         -9.9997e-01, -9.9548e-01,  9.7109e-01,  9.9973e-01,  1.0000e+00,\n",
       "          9.9962e-01,  9.8807e-01, -1.3277e-01, -9.9998e-01,  8.1821e-01,\n",
       "         -1.1086e-01, -2.4992e-01, -2.1118e-01, -9.4118e-01, -9.9998e-01,\n",
       "          9.9998e-01, -9.9998e-01, -9.9999e-01, -9.9439e-01, -1.0000e+00,\n",
       "          9.9972e-01,  6.0120e-01,  9.9976e-01,  9.1483e-01, -9.9994e-01,\n",
       "         -9.9865e-01, -1.1880e-01, -9.3530e-01, -9.9910e-01,  3.7715e-02,\n",
       "         -1.0000e+00, -3.6627e-02,  1.5413e-01, -8.6804e-01,  7.8122e-02,\n",
       "         -9.2812e-01, -5.9431e-01,  9.9960e-01,  7.8203e-01,  9.9191e-01,\n",
       "         -9.9789e-01, -9.9915e-01,  1.4862e-01, -9.9999e-01,  9.9511e-01,\n",
       "          9.9995e-01, -1.8118e-01,  9.6320e-01, -9.0529e-01,  2.0719e-01,\n",
       "         -9.9999e-01, -1.0000e+00,  8.9915e-01,  9.9988e-01, -9.4724e-02,\n",
       "         -9.9991e-01, -5.8353e-02, -9.9938e-01, -2.2529e-01,  9.8906e-01,\n",
       "          9.9867e-01, -9.9953e-01,  9.9986e-01, -9.9518e-01,  1.2698e-02,\n",
       "          9.9326e-01, -1.0000e+00,  8.2947e-01, -9.9548e-01,  1.0000e+00,\n",
       "         -1.0000e+00,  9.8128e-01, -2.5718e-01, -1.9399e-01,  6.8583e-02,\n",
       "          8.4061e-01, -9.9994e-01, -1.8220e-02,  9.8823e-01,  9.9371e-01,\n",
       "         -5.0792e-02,  9.9943e-01, -4.9444e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[4.7839e-01, 3.7087e-04, 1.6194e-04,  ..., 1.4241e-04,\n",
       "           4.1822e-04, 5.1812e-01],\n",
       "          [5.0953e-03, 8.7933e-02, 1.0557e-01,  ..., 1.7039e-01,\n",
       "           1.4170e-01, 3.9708e-03],\n",
       "          [1.6069e-02, 8.9643e-02, 1.5971e-01,  ..., 9.8671e-02,\n",
       "           1.9282e-01, 2.9035e-03],\n",
       "          ...,\n",
       "          [2.8493e-02, 8.8892e-02, 4.6252e-02,  ..., 1.1091e-01,\n",
       "           2.2474e-01, 1.1121e-02],\n",
       "          [6.0998e-02, 5.6417e-02, 4.1991e-02,  ..., 7.0832e-02,\n",
       "           3.5807e-01, 2.4441e-02],\n",
       "          [4.6692e-01, 9.0001e-04, 4.2503e-04,  ..., 2.7258e-04,\n",
       "           1.1382e-03, 5.2722e-01]],\n",
       "\n",
       "         [[9.8987e-01, 2.5771e-05, 2.0233e-04,  ..., 5.2055e-05,\n",
       "           1.0738e-03, 3.7624e-03],\n",
       "          [1.7755e-02, 1.2910e-04, 9.8209e-01,  ..., 2.9791e-10,\n",
       "           2.1697e-06, 2.7124e-07],\n",
       "          [2.1831e-02, 9.7443e-01, 1.5326e-04,  ..., 1.2469e-08,\n",
       "           7.8246e-08, 1.0876e-04],\n",
       "          ...,\n",
       "          [3.0362e-02, 7.9185e-09, 1.0954e-07,  ..., 3.9590e-04,\n",
       "           8.5866e-03, 4.6615e-04],\n",
       "          [2.4575e-01, 2.9064e-06, 6.4724e-08,  ..., 3.6930e-03,\n",
       "           4.7571e-04, 7.4960e-01],\n",
       "          [5.9083e-01, 1.3708e-07, 7.4790e-05,  ..., 2.6530e-05,\n",
       "           4.0550e-01, 2.6670e-03]],\n",
       "\n",
       "         [[1.5828e-01, 8.7503e-02, 2.4697e-02,  ..., 5.4641e-02,\n",
       "           9.3273e-02, 2.2118e-01],\n",
       "          [4.7261e-01, 3.2207e-01, 3.1096e-02,  ..., 8.9683e-03,\n",
       "           1.6882e-02, 9.5203e-02],\n",
       "          [4.8853e-01, 1.7710e-01, 1.9191e-01,  ..., 4.7729e-03,\n",
       "           1.3719e-02, 6.2083e-02],\n",
       "          ...,\n",
       "          [1.5848e-01, 6.2461e-02, 5.9279e-02,  ..., 6.9960e-02,\n",
       "           3.0103e-02, 3.0245e-02],\n",
       "          [6.0439e-02, 4.6028e-02, 2.6160e-02,  ..., 4.8402e-02,\n",
       "           9.6760e-02, 9.5885e-02],\n",
       "          [3.6124e-02, 3.9422e-02, 4.3673e-02,  ..., 8.1842e-02,\n",
       "           1.4326e-01, 1.2542e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[7.1624e-01, 2.2281e-02, 4.1150e-02,  ..., 2.4048e-02,\n",
       "           2.4155e-02, 7.6339e-02],\n",
       "          [4.0876e-01, 3.2392e-01, 1.1157e-02,  ..., 1.2521e-02,\n",
       "           1.7772e-02, 1.0539e-01],\n",
       "          [5.7864e-01, 5.4866e-03, 5.3609e-03,  ..., 1.7091e-02,\n",
       "           2.5296e-02, 2.7067e-01],\n",
       "          ...,\n",
       "          [3.8113e-01, 4.5102e-02, 5.0164e-02,  ..., 2.4956e-01,\n",
       "           2.1874e-02, 7.9774e-02],\n",
       "          [1.2497e-01, 1.6161e-02, 3.0162e-02,  ..., 2.5589e-02,\n",
       "           4.0640e-01, 2.9727e-01],\n",
       "          [4.2541e-01, 4.2506e-02, 8.8982e-02,  ..., 1.3463e-02,\n",
       "           2.0428e-02, 1.4088e-01]],\n",
       "\n",
       "         [[9.7282e-01, 5.4095e-03, 3.0819e-03,  ..., 1.6773e-03,\n",
       "           2.3250e-03, 5.3986e-03],\n",
       "          [7.9572e-03, 7.2623e-02, 4.8053e-01,  ..., 4.1826e-03,\n",
       "           1.9901e-03, 6.7593e-03],\n",
       "          [1.4960e-03, 7.2051e-03, 1.7665e-02,  ..., 6.2034e-04,\n",
       "           4.2365e-03, 2.2028e-03],\n",
       "          ...,\n",
       "          [1.7114e-03, 4.9418e-04, 1.8217e-04,  ..., 6.1058e-03,\n",
       "           8.0612e-01, 1.7840e-01],\n",
       "          [2.0528e-02, 1.3975e-03, 1.1164e-03,  ..., 7.0097e-03,\n",
       "           1.0093e-01, 8.5835e-01],\n",
       "          [9.9353e-01, 6.1979e-05, 1.0303e-04,  ..., 2.0507e-04,\n",
       "           6.5383e-04, 5.1244e-03]],\n",
       "\n",
       "         [[4.1435e-01, 2.3849e-02, 1.9253e-02,  ..., 1.5636e-02,\n",
       "           3.1795e-02, 3.2332e-01],\n",
       "          [7.6895e-01, 1.1215e-01, 2.3229e-03,  ..., 4.8616e-03,\n",
       "           1.7126e-02, 6.9241e-03],\n",
       "          [4.3983e-02, 9.3144e-01, 6.5222e-03,  ..., 1.7750e-04,\n",
       "           1.1039e-03, 1.4449e-03],\n",
       "          ...,\n",
       "          [4.8826e-02, 1.2608e-03, 8.5690e-05,  ..., 3.0950e-02,\n",
       "           1.3415e-02, 7.6230e-03],\n",
       "          [3.2924e-01, 1.0628e-02, 1.2599e-02,  ..., 2.6462e-01,\n",
       "           1.2657e-01, 4.7936e-02],\n",
       "          [7.0947e-02, 1.5121e-03, 7.2978e-04,  ..., 2.2052e-02,\n",
       "           3.8989e-01, 5.0019e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.3373e-01, 1.2245e-02, 5.9640e-03,  ..., 6.6840e-03,\n",
       "           5.9404e-02, 4.1570e-01],\n",
       "          [4.9442e-01, 1.5818e-02, 3.7789e-03,  ..., 2.2111e-06,\n",
       "           7.0639e-03, 4.7135e-01],\n",
       "          [1.5036e-02, 9.6916e-01, 1.7162e-05,  ..., 2.2350e-07,\n",
       "           8.1731e-06, 1.4880e-02],\n",
       "          ...,\n",
       "          [2.7958e-02, 9.5082e-06, 1.2319e-05,  ..., 4.3027e-05,\n",
       "           7.6985e-04, 2.6946e-02],\n",
       "          [2.2754e-01, 4.7425e-04, 2.5739e-04,  ..., 4.9793e-01,\n",
       "           2.7242e-02, 2.2332e-01],\n",
       "          [4.3523e-01, 1.1644e-02, 5.8632e-03,  ..., 6.3988e-03,\n",
       "           6.0729e-02, 4.1686e-01]],\n",
       "\n",
       "         [[4.6542e-01, 1.0083e-02, 6.0318e-03,  ..., 6.6627e-03,\n",
       "           1.0336e-02, 4.4863e-01],\n",
       "          [2.5495e-01, 1.3774e-02, 1.1257e-02,  ..., 5.7545e-02,\n",
       "           1.1857e-01, 2.5624e-01],\n",
       "          [2.0169e-01, 2.1767e-02, 8.0970e-03,  ..., 8.4209e-02,\n",
       "           1.5188e-01, 2.0265e-01],\n",
       "          ...,\n",
       "          [3.1249e-01, 2.5190e-02, 9.0194e-03,  ..., 6.2476e-02,\n",
       "           1.1004e-01, 3.1861e-01],\n",
       "          [1.1392e-01, 2.7426e-02, 1.3608e-02,  ..., 1.1509e-01,\n",
       "           2.2060e-01, 1.1615e-01],\n",
       "          [4.6755e-01, 9.4318e-03, 5.6914e-03,  ..., 6.3617e-03,\n",
       "           9.6006e-03, 4.5107e-01]],\n",
       "\n",
       "         [[4.9466e-01, 4.5281e-03, 4.1881e-03,  ..., 2.1171e-03,\n",
       "           4.5976e-03, 4.5300e-01],\n",
       "          [3.8351e-01, 2.3337e-01, 1.0258e-02,  ..., 1.5342e-03,\n",
       "           6.8763e-04, 3.5521e-01],\n",
       "          [3.7723e-01, 1.2363e-01, 8.9903e-02,  ..., 1.3421e-03,\n",
       "           2.5951e-03, 3.5213e-01],\n",
       "          ...,\n",
       "          [4.1745e-01, 4.6958e-03, 3.4181e-04,  ..., 1.0713e-01,\n",
       "           9.9782e-03, 3.9401e-01],\n",
       "          [2.4654e-01, 1.1436e-03, 1.3578e-03,  ..., 2.7901e-03,\n",
       "           4.8146e-01, 2.3514e-01],\n",
       "          [4.9448e-01, 4.7151e-03, 4.3291e-03,  ..., 2.1608e-03,\n",
       "           4.6035e-03, 4.5232e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.5297e-01, 9.7434e-03, 1.9005e-02,  ..., 1.3204e-02,\n",
       "           2.3992e-02, 4.2695e-01],\n",
       "          [2.7961e-01, 1.5836e-02, 2.7112e-02,  ..., 1.5471e-02,\n",
       "           2.0074e-02, 2.7947e-01],\n",
       "          [2.5138e-01, 7.8463e-03, 1.1442e-02,  ..., 1.5963e-02,\n",
       "           2.1815e-02, 2.5865e-01],\n",
       "          ...,\n",
       "          [4.5071e-01, 1.7168e-03, 1.4448e-03,  ..., 2.0691e-02,\n",
       "           4.9900e-02, 4.6322e-01],\n",
       "          [4.3516e-01, 1.8440e-02, 8.2644e-03,  ..., 8.7199e-03,\n",
       "           4.7606e-02, 4.3559e-01],\n",
       "          [4.5474e-01, 9.4233e-03, 1.8301e-02,  ..., 1.2543e-02,\n",
       "           2.2918e-02, 4.2902e-01]],\n",
       "\n",
       "         [[4.8773e-01, 4.6222e-03, 2.2272e-03,  ..., 1.8819e-03,\n",
       "           4.9725e-03, 4.5880e-01],\n",
       "          [4.4086e-01, 1.9669e-02, 1.3166e-02,  ..., 5.0598e-03,\n",
       "           1.0383e-02, 4.3453e-01],\n",
       "          [3.3994e-01, 4.4037e-02, 2.8779e-02,  ..., 1.4944e-02,\n",
       "           3.1276e-02, 3.2621e-01],\n",
       "          ...,\n",
       "          [2.1561e-01, 4.0348e-02, 2.9914e-02,  ..., 1.3212e-02,\n",
       "           6.5098e-02, 2.1214e-01],\n",
       "          [1.3515e-01, 4.7511e-02, 4.1154e-02,  ..., 2.1351e-02,\n",
       "           7.9080e-02, 1.3596e-01],\n",
       "          [4.8721e-01, 4.6815e-03, 2.2519e-03,  ..., 1.9154e-03,\n",
       "           4.8883e-03, 4.5870e-01]],\n",
       "\n",
       "         [[4.4423e-01, 6.6496e-03, 1.5502e-02,  ..., 2.1103e-03,\n",
       "           8.0779e-03, 4.3109e-01],\n",
       "          [3.1020e-01, 7.7069e-02, 6.7852e-02,  ..., 3.9983e-02,\n",
       "           3.9443e-02, 3.0346e-01],\n",
       "          [3.4047e-01, 9.1359e-02, 4.0578e-02,  ..., 2.2853e-02,\n",
       "           1.3726e-02, 3.3106e-01],\n",
       "          ...,\n",
       "          [2.8709e-01, 1.6954e-01, 1.7326e-02,  ..., 1.8816e-02,\n",
       "           2.6756e-02, 2.8218e-01],\n",
       "          [2.0717e-01, 8.9429e-02, 2.8123e-02,  ..., 2.1913e-02,\n",
       "           3.0647e-02, 2.0473e-01],\n",
       "          [4.4307e-01, 6.4938e-03, 1.5577e-02,  ..., 2.0572e-03,\n",
       "           7.9578e-03, 4.3017e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.0857e-01, 3.4843e-02, 2.9431e-02,  ..., 2.2023e-02,\n",
       "           9.3524e-02, 3.0434e-01],\n",
       "          [4.6369e-01, 3.3651e-02, 1.8516e-02,  ..., 1.8411e-03,\n",
       "           3.9301e-03, 4.5764e-01],\n",
       "          [4.7998e-01, 1.9158e-02, 9.4337e-03,  ..., 1.5937e-03,\n",
       "           3.5418e-03, 4.7203e-01],\n",
       "          ...,\n",
       "          [2.0987e-01, 7.0485e-02, 2.4167e-02,  ..., 6.2587e-03,\n",
       "           1.4289e-02, 2.0687e-01],\n",
       "          [1.8406e-01, 3.3503e-02, 2.5167e-02,  ..., 1.8901e-02,\n",
       "           5.3032e-02, 1.8182e-01],\n",
       "          [3.0962e-01, 3.4783e-02, 2.9335e-02,  ..., 2.1975e-02,\n",
       "           9.2865e-02, 3.0539e-01]],\n",
       "\n",
       "         [[1.6099e-02, 5.5865e-02, 1.9955e-02,  ..., 8.2577e-02,\n",
       "           2.4152e-01, 1.6014e-02],\n",
       "          [3.1265e-01, 9.2869e-02, 2.1438e-01,  ..., 7.2339e-03,\n",
       "           1.6272e-03, 3.0860e-01],\n",
       "          [7.6654e-02, 8.3793e-01, 2.8228e-03,  ..., 1.0462e-04,\n",
       "           1.2289e-04, 7.6044e-02],\n",
       "          ...,\n",
       "          [1.9498e-01, 1.1524e-02, 2.7639e-03,  ..., 1.5378e-02,\n",
       "           6.1970e-02, 1.9332e-01],\n",
       "          [9.8355e-02, 1.5854e-01, 1.7636e-02,  ..., 2.2231e-01,\n",
       "           3.5096e-02, 9.7422e-02],\n",
       "          [1.6208e-02, 5.6137e-02, 2.0051e-02,  ..., 8.2288e-02,\n",
       "           2.4146e-01, 1.6122e-02]],\n",
       "\n",
       "         [[4.3342e-01, 2.3085e-03, 9.0360e-04,  ..., 8.1337e-03,\n",
       "           8.3780e-02, 4.3071e-01],\n",
       "          [4.1979e-01, 3.9501e-02, 4.3202e-02,  ..., 2.8765e-03,\n",
       "           9.3446e-04, 4.1249e-01],\n",
       "          [4.2832e-01, 2.0849e-02, 6.6290e-02,  ..., 9.1756e-03,\n",
       "           2.0217e-03, 4.2133e-01],\n",
       "          ...,\n",
       "          [3.9139e-01, 5.5616e-03, 5.0340e-03,  ..., 6.4238e-02,\n",
       "           1.9087e-02, 3.8480e-01],\n",
       "          [3.4569e-01, 2.6154e-02, 2.5267e-02,  ..., 4.9495e-02,\n",
       "           2.5121e-02, 3.4260e-01],\n",
       "          [4.3337e-01, 2.3314e-03, 9.0538e-04,  ..., 8.1911e-03,\n",
       "           8.3627e-02, 4.3067e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[3.5889e-02, 1.0875e-01, 6.2260e-02,  ..., 1.3727e-01,\n",
       "           8.6740e-02, 3.5555e-02],\n",
       "          [3.2998e-01, 4.0057e-02, 4.4795e-02,  ..., 3.5298e-02,\n",
       "           4.0687e-03, 3.2672e-01],\n",
       "          [3.3202e-01, 4.6843e-02, 2.1038e-02,  ..., 2.1147e-02,\n",
       "           5.7304e-03, 3.2798e-01],\n",
       "          ...,\n",
       "          [4.9270e-01, 6.5295e-04, 2.1087e-04,  ..., 5.7735e-03,\n",
       "           5.0478e-03, 4.9033e-01],\n",
       "          [4.6342e-01, 7.8648e-03, 6.6891e-03,  ..., 1.6827e-02,\n",
       "           1.9917e-02, 4.6089e-01],\n",
       "          [3.6317e-02, 1.0910e-01, 6.2330e-02,  ..., 1.3684e-01,\n",
       "           8.6587e-02, 3.5979e-02]],\n",
       "\n",
       "         [[4.4464e-02, 4.2673e-02, 5.8829e-02,  ..., 1.0975e-01,\n",
       "           2.3885e-01, 4.4236e-02],\n",
       "          [4.8661e-01, 1.9577e-02, 5.4720e-03,  ..., 5.2087e-04,\n",
       "           5.3391e-04, 4.8192e-01],\n",
       "          [4.4674e-01, 7.0038e-02, 2.9482e-02,  ..., 3.6914e-04,\n",
       "           1.1172e-03, 4.4032e-01],\n",
       "          ...,\n",
       "          [5.7166e-02, 1.1684e-02, 5.7533e-03,  ..., 9.0282e-03,\n",
       "           1.0928e-02, 5.6165e-02],\n",
       "          [1.5224e-01, 2.6014e-02, 1.6841e-02,  ..., 3.2682e-02,\n",
       "           4.0982e-02, 1.4985e-01],\n",
       "          [4.4650e-02, 4.2422e-02, 5.8535e-02,  ..., 1.1047e-01,\n",
       "           2.3935e-01, 4.4420e-02]],\n",
       "\n",
       "         [[1.5456e-01, 4.0307e-02, 2.3888e-02,  ..., 6.2591e-02,\n",
       "           2.1191e-01, 1.5159e-01],\n",
       "          [3.2368e-01, 1.0255e-02, 1.3362e-01,  ..., 1.2689e-02,\n",
       "           9.0063e-03, 3.2014e-01],\n",
       "          [3.6199e-01, 5.2327e-03, 8.3667e-03,  ..., 8.9195e-04,\n",
       "           3.4042e-03, 3.5831e-01],\n",
       "          ...,\n",
       "          [4.5667e-01, 1.0435e-03, 1.0622e-03,  ..., 6.0683e-03,\n",
       "           7.0331e-02, 4.5400e-01],\n",
       "          [4.1754e-01, 3.5981e-02, 9.1465e-03,  ..., 3.9345e-02,\n",
       "           3.1219e-02, 4.1797e-01],\n",
       "          [1.5569e-01, 4.0257e-02, 2.3803e-02,  ..., 6.1907e-02,\n",
       "           2.1164e-01, 1.5270e-01]]]], grad_fn=<SoftmaxBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 帶Model Head的模型調用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clz_model = AutoModelForSequenceClassification.from_pretrained(location, num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2932, -0.0113,  0.2361, -0.3064,  0.0161, -0.0471, -0.4812, -0.3675,\n",
       "          0.4133,  0.5009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clz_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clz_model.config.num_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
