{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相關參考\n",
    "- [hugginface CSV](https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path)\n",
    "- [official datasets introduction](https://www.kaggle.com/code/nbroad/intro-to-hugging-face-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch, pytorch\n",
    "HuggingFace的datasets库中load_dataset方法使用\n",
    "\n",
    "\n",
    "datasets是抱抱脸开发的一个数据集python库，可以很方便的从Hugging Face Hub里下载数据，也可很方便的从本地加载数据集，本文主要对load_dataset方法的使用进行详细说明\n",
    "\n",
    "## load_dataset参数\n",
    "load_dataset有以下参数，具体可参考  \n",
    "\n",
    "```python\n",
    "def load_dataset(\n",
    "    path: str,\n",
    "    name: Optional[str] = None,\n",
    "    data_dir: Optional[str] = None,\n",
    "    data_files: Union[Dict, List] = None,\n",
    "    split: Optional[Union[str, Split]] = None,\n",
    "    cache_dir: Optional[str] = None,\n",
    "    features: Optional[Features] = None,\n",
    "    download_config: Optional[DownloadConfig] = None,\n",
    "    download_mode: Optional[GenerateMode] = None,\n",
    "    ignore_verifications: bool = False,\n",
    "    save_infos: bool = False,\n",
    "    script_version: Optional[Union[str, Version]] = None,\n",
    "    **config_kwargs,\n",
    ") -> Union[DatasetDict, Dataset]:\n",
    "``` \n",
    "- path：参数path表示数据集的名字或者路径。可以是如下几种形式（每种形式的使用方式后面会详细说明）\n",
    "  - 数据集的名字，比如imdb、glue\n",
    "  - 数据集文件格式，比如json、csv、parquet、txt\n",
    "  - 数据集目录中的处理数据集的脚本（.py)文件，比如“glue/glue.py”\n",
    "- name：参数name表示数据集中的子数据集，当一个数据集包含多个数据集时，就需要这个参数，比如- glue数据集下就包含\"sst2\"、“cola”、\"qqp\"等多个子数据集，此时就需要指定name来表示加载哪一个子数据集\n",
    "- data_dir：数据集所在的目录\n",
    "- data_files：数据集文件\n",
    "- cache_dir：构建的数据集缓存目录，方便下次快速加载\n",
    "以上为一些常用且比较重要的参数，其他参数很少用到因此在此处不再详细说明，下面会通过一些case更加具体的说明各种用法\n",
    "\n",
    "#### 详细用法\n",
    "从HuggingFace Hub上加载数据\n",
    "\n",
    "首先我们可以通过如下方式查看Hubs上有哪些数据集\n",
    "```python\n",
    "from datasets import list_datasets\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "print( len(datasets_list))\n",
    "print(datasets_list[:10])\n",
    "``` \n",
    "\n",
    "输出如下\n",
    "\n",
    "```txt\n",
    "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n",
    "```\n",
    "\n",
    "后面通过直接指定path等于相关数据集的名字就能下载并加载相关数据集\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(path='squad', split='train')\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試1\n",
    "從CSV中載入,第一ROW有欄位名稱,但是split=true沒用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['f1', 'f2', 'f3', 'f4', 'f5'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import *\n",
    "dataset = load_dataset(\"csv\",data_files=\"../../../python/dataset/iris2.csv\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['f1', 'f2', 'f3', 'f4', 'f5'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import *\n",
    "ds = load_dataset(\"csv\",data_files=\"../../../python/dataset/iris2.csv\")\n",
    "ds['train'] ## 和ds結果一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['f1', 'f2', 'f3', 'f4', 'f5'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['f1', 'f2', 'f3', 'f4', 'f5'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 150*0.2=30\n",
    "train_testvalid = ds['train'].train_test_split(test_size=0.2)\n",
    "train_testvalid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
