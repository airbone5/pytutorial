{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Back propagation is a fundamental technique used in the training of neural networks which helps in optimizing the weights and biases of a model based on the error between the predicted output and the actual output. The basic idea behind this technique is to calculate the gradient of the loss function with respect to each weight and bias in the model. The gradient tells us how much the loss function will be affected by changing the weights and bias by a small amount. The main goal is to reduce the loss which is achieved by iteratively updating the weights and bias of the model based on the gradient.\n",
    "\n",
    "Backpropagation consists of two phases - the first one is a feedforward pass and the later is a backward pass where the weights and bias are optimized.\n",
    "\n",
    "### Feedforward Pass:\n",
    "This is the first step in the training of a neural network where the data flows from the input layer to the output layer through certain hidden layers, undergoing essential computations. Neurons in each layer perform weighted sum calculations, and apply activation functions, capturing intricate data patterns. Hidden layers transform the data into hierarchical features, aiding in understanding complex structures. The process culminates at the output layer, producing predictions or classifications. During training, neural networks optimize weights and biases through backpropagation, enhancing their predictive accuracy. This process, combined with feedforward pass, empowers neural networks to learn and excel in various applications.\n",
    "\n",
    "### Backward Pass:\n",
    "The backward pass is a critical phase in neural network training, initiated after making predictions to minimize errors and enhance accuracy. It calculates the disparity between actual and predicted values, aiming to reduce this error. In this phase, error information is retroactively propagated from the output layer to the input layer. The key objective is to compute gradients with respect to the network's weights and biases. These gradients reveal the contribution of each weight and bias to the error, helping the network understand how to adjust parameters to minimize errors systematically. Through backpropagation, neural networks iteratively fine-tune their parameters, ultimately improving their predictive capabilities.\n",
    "\n",
    "Then the weights get updated, and both the passes run iteratively till we get reduced loss.\n",
    "\n",
    "### Back propagation in TensorFlow\n",
    "TensorFlow is one of the most popular deep learning libraries which helps in efficient training of deep neural networks. Now let's deep dive into how back propagation works in TensorFlow.\n",
    "\n",
    "In tensorflow, back propagation is calculated using automatic differentiation, which is a technique where we don't explicitly compute the gradients of the function. When we define the neural network, tensorflow automatically creates a computational graph that represents the flow of data through the network. Each node consists of the mathematical operation that takes place during both the forward as well as backward pass.\n",
    "\n",
    "The goal of back propagation is to optimize the weights and biases of the model to minimize the loss. So, we use tensorflow's automatic differentiation capabilities to compute the gradient of the loss function with respect to weights and biases. When the variable is defined, its takes a trainable parameter which can be set to True, which tells TensorFlow to keep track of its value during training and compute its gradient with respect to the loss function.\n",
    "\n",
    "Once we have the gradients, there are certain optimizers in Tensorflow such as SGD, Adagrad, and Adam which can be used to update the weights accordingly.\n",
    "\n",
    "#### Implementing Back propagation\n",
    "Installing of the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First install tensorflow in your system by entering the command in your terminal\n",
    "\n",
    "**Importing Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we are importing all the important libraries need to build the model.\n",
    "The following libraries are:\n",
    "\n",
    "- Numpy: A Python library for numerical computations, including support for large, multi-dimensional arrays and matrices, along with a wide array of mathematical functions.\n",
    "- Sklearn: A python library for machine learning that provides tools for data preprocessing, modelling, evaluation, and various algorithms for classification, regression, and more.\n",
    "Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Extract the features (X) and target labels (y) from the dataset\n",
    "# X contains the feature data\n",
    "X = iris.data\n",
    "# y contains the target labels\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here in this Code, we are gathering the data and preprocessing it. Preprocessing of the data require cleaning of data, removal of outliers, and if the numerical data is huge then scaling it to a specific range. In order to study the model , spilt the prepared data into training and testing data.\n",
    "\n",
    "Training and Testing the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, We divide the the iris dataset into training set (80%) and testing set(20%) to facilitate the development and evaluation of the model. The 'random_state' argument is set for reproducibility, ensuring that same split is obtained each time the code is run.\n",
    "\n",
    "**Defining a machine learning model**\n",
    "\n",
    "Here, we are defining a model using tensorflow's Keras.\n",
    "The model consist of two layers:\n",
    "\n",
    "Dense Layer: A hidden layer with the ReLU activation function and an input shape that matches the number of the features in the training data.\n",
    "Output Layer: An output layer has three neurons and uses a softmax activation function to produce class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pywork\\ollama\\basic5_torch\\prj\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">259</span> (1.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m259\u001b[0m (1.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">259</span> (1.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m259\u001b[0m (1.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the neural network architecture 兩層,輸出層有3個神經元\n",
    "hidden_layer_size =32\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes for Iris dataset\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function and optimizer**\n",
    "\n",
    "Here we are defining the loss function and optimizer used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "hidden_layer_size = 10\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sparse Categorical Crossentropy: It is a loss function used in classification tasks where target and labels are integers. It calculates the cross-entropy loss between the predicted class probabilities and true class labels, automatically converting integer labels to one-hot encoded vectors internally.\n",
    "- Stochastic Gradient Descent(SGD): It is an optimization algorithm used for training models. It updates model parameters using small, randomly sampled subsets of the training data, which introduces randomness and helps the model converge to a solution faster and potentially escape local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< notice type=\"info\" class=\"\" >}}\n",
    "**Categorical Crossentropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "y_true = np.array([1, 2])  # 原始資料為整數分類\n",
    "y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])  # 預測概率\n",
    "delta = 1e-7\n",
    "y_true_one_hot = tf.keras.utils.to_categorical( # 獨熱編碼\n",
    "    y_true,  # 類數組，類值要轉換為矩陣（從 0 到 分類數-1 的整數）\n",
    "    num_classes=3,  # 分類數\n",
    "    dtype=\"float32\" # 輸出數據類型，默認為float32\n",
    ")\n",
    "print(y_true_one_hot)\n",
    "```\n",
    "```\n",
    "[[0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "``` \n",
    "![](tensorflow_bpn.files/2024-12-02-16-56-56.png)\n",
    "\n",
    "#### 利用tf.keras.losses.SparseCategoricalCrossentropy 實現\n",
    "```python\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "print(scce(y_true, y_pred).numpy())\n",
    "```\n",
    "```\n",
    "1.1769392490386963\n",
    "```\n",
    "\n",
    "#### numpy 實現\n",
    "```python\n",
    "y_pred_delta = y_pred+delta  # 添加一個微小值可以防止負無限大(np.log(0))的發生。\n",
    "print(y_pred_delta)\n",
    "\n",
    "# [[5.0e-02 9.5e-01 1.0e-11]\n",
    "#  [1.0e-01 8.0e-01 1.0e-01]]\n",
    "\n",
    "y_pred_log = np.log(y_pred_delta)  # log表示以e為底數的自然對數\n",
    "print(y_pred_log)\n",
    "\n",
    "# [[ -2.99573227  -0.05129329 -25.32843602]\n",
    "#  [ -2.30258509  -0.22314355  -2.30258509]]\n",
    "\n",
    "print(y_true_one_hot*y_pred_log)\n",
    "\n",
    "# [[-0.         -0.05129329 -0.        ]\n",
    "#  [-0.         -0.         -2.30258509]]\n",
    "\n",
    "loss = -np.sum(y_true_one_hot*y_pred_log)/2\n",
    "print(loss)\n",
    "\n",
    "# 1.1769392490386963\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< /notice >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "Now implement the backpropagation on the trained model in a loop called training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.5040257573127747\n",
      "Epoch 200/1000, Loss: 0.3756517469882965\n",
      "Epoch 300/1000, Loss: 0.31077930331230164\n",
      "Epoch 400/1000, Loss: 0.26728513836860657\n",
      "Epoch 500/1000, Loss: 0.23448063433170319\n",
      "Epoch 600/1000, Loss: 0.20877058804035187\n",
      "Epoch 700/1000, Loss: 0.18835049867630005\n",
      "Epoch 800/1000, Loss: 0.17195174098014832\n",
      "Epoch 900/1000, Loss: 0.15864413976669312\n",
      "Epoch 1000/1000, Loss: 0.1477571427822113\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Iterate through a specified number of training epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Use TensorFlow's GradientTape to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions (logits) by passing training data through the neural network\n",
    "        logits = model(X_train)\n",
    "\n",
    "        # Calculate the loss by comparing predicted logits with the true training labels (y_train)\n",
    "        loss_value = loss_fn(y_train, logits)\n",
    "\n",
    "    # Backpropagation: Compute gradients of the loss with respect to model parameters\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    # Apply the computed gradients to update the model's parameters using the specified optimizer\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Print the loss at regular intervals to monitor training progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_value.numpy()}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, it represents a training loop for a neural network. It iterates through a specified number of epochs, computing predictions, calculating predictions and loss, and updating model parameters using backpropagation and an optimizer. Training progress is monitored by printing the loss every 100 epochs.\n",
    "Clearly, with increasing epochs the loss is gradually decreasing. This is a result of backpropagation, its adjusting the weights of layers according to the desired output in order to achieve higher accuracy.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "Efficient Gradient Calculation: Tensorflow's automatic differentiation capabilities make it efficient to compute gradients during backpropagation. This is crucial for optimization the mode's parameters.\n",
    "Flexibility: Tensorflow allows you to define and customize complex neural network architectures easily, making it suitable for wide range of ML tasks.\n",
    "\n",
    "GPU Acceleration: Tensorflow effortlessly integrates with GPUs, which can significantly speed up the training process for neural networks.\n",
    "Deployment: Tensorflow provides tools for converting trained models into formats suitable for deployment on various platforms, including mobile devices and the web.\n",
    "\n",
    "** Disadvantages**\n",
    "\n",
    "Increased memory consumption: It requires storing the intermediate values during forward and backward passes to complete gradients.\n",
    "Computational overhead: Using Automatic differentiation for simple functions can create a significant computational overhead. So, its better to generate gradients manually for these functions.\n",
    "\n",
    "Updates and Compatibility: Tensorflow occasionally introduces updates and changes that may require adjustments to existing code. Compatibility with older versions can be a concern for long term projects.\n",
    "Resource Intensive: Training deep neural networks with tensorflow can be resource intensive, requiring powerful GPUs or TPUs, which may not be readily available to everyone.\n",
    "\n",
    "Are you passionate about data and looking to make one giant leap into your career? Our Data Science Course will help you change your game and, most importantly, allow students, professionals, and working adults to tide over into the data science immersion. Master state-of-the-art methodologies, powerful tools, and industry best practices, hands-on projects, and real-world applications. Become the executive head of industries related to Data Analysis, Machine Learning, and Data Visualization with these growing skills. Ready to Transform Your Future? Enroll Now to Be a Data Science Expert!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
